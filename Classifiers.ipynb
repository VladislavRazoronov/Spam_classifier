{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of my project is to implement an efficient and universal spam classifier. For spam classifier most important metrics are general accuracy and positive negative count. For the testing I'm going to use email and SMS spam databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_data.csv\")\n",
    "data1 = pd.read_csv(\"spam.csv\",encoding='latin1')\n",
    "data2 = pd.read_csv(\"spam_ham_dataset.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2, random_state=0)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data1[\"v2\"], data1[\"v1\"], test_size=0.2, random_state=0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data2[\"text\"], data2[\"label\"], test_size=0.2, random_state=0)\n",
    "X_all = pd.concat([X_train,X_train1,X_train2])\n",
    "Y_all = pd.concat([y_train,y_train1,y_train2])\n",
    "cv = TfidfVectorizer() \n",
    "X_all = cv.fit_transform(X_all)\n",
    "X_train = cv.transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "X_train1 = cv.transform(X_train1)\n",
    "X_test1 = cv.transform(X_test1)\n",
    "X_train2 = cv.transform(X_train2)\n",
    "X_test2 = cv.transform(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets consist of email/sms messages with ham/spam labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = naive_bayes.MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.partial_fit(X_train1,y_train1)\n",
    "model.partial_fit(X_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict1 = model.predict(X_test1)\n",
    "y_predict2 = model.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1 correct guesses: 6630 / 6744 : 98.30960854092527 %\n",
      "Dataset2 correct guesses: 975 / 1115 : 87.4439461883408 %\n",
      "Dataset3 correct guesses: 1014 / 1035 : 97.97101449275362 %\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test.iteritems():\n",
    "    if(y_predict[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset1 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test1.iteritems():\n",
    "    if(y_predict1[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset2 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test2.iteritems():\n",
    "    if(y_predict2[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset3 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ds1 f1 macro = 0.98\n",
      "Ds1 f1 micro = 0.98\n",
      "Ds1 f1 weighted = 0.98\n",
      "Ds2 f1 macro = 0.79\n",
      "Ds2 f1 micro = 0.87\n",
      "Ds2 f1 weighted = 0.88\n",
      "Ds3 f1 macro = 0.98\n",
      "Ds3 f1 micro = 0.98\n",
      "Ds3 f1 weighted = 0.98\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_predict, average='macro')  \n",
    "print('Ds1 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test, y_predict, average='micro')  \n",
    "print('Ds1 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test, y_predict, average='weighted')  \n",
    "print('Ds1 f1 weighted = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='macro')  \n",
    "print('Ds2 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='micro')  \n",
    "print('Ds2 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='weighted')  \n",
    "print('Ds2 f1 weighted = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='macro')  \n",
    "print('Ds3 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='micro')  \n",
    "print('Ds3 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='weighted')  \n",
    "print('Ds3 f1 weighted = %.2f' %(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ds1 False positive:  12 ; 0.1779359430604982 %\n",
      "Ds2 False positive:  114 ; 10.22421524663677 %\n",
      "Ds3 False positive:  8 ; 0.7729468599033816 %\n"
     ]
    }
   ],
   "source": [
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test.iteritems():\n",
    "    if y_predict[idx]!=item and y_predict[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds1 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test1.iteritems():\n",
    "    if y_predict1[idx]!=item and y_predict1[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds2 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test2.iteritems():\n",
    "    if y_predict2[idx]!=item and y_predict2[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds3 False positive: \",fp,\";\",(fp/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 1.0\n",
    "svc = svm.SVC(kernel='linear', C=C)\n",
    "svc.fit(X_all,Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = svc.predict(X_test)\n",
    "y_predict1 = svc.predict(X_test1)\n",
    "y_predict2 = svc.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1 correct guesses: 6679 / 6744 : 99.0361803084223 %\n",
      "Dataset2 correct guesses: 1067 / 1115 : 95.69506726457399 %\n",
      "Dataset3 correct guesses: 1030 / 1035 : 99.51690821256038 %\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test.iteritems():\n",
    "    if(y_predict[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset1 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test1.iteritems():\n",
    "    if(y_predict1[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset2 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test2.iteritems():\n",
    "    if(y_predict2[idx]==item):\n",
    "        score+=1\n",
    "    idx += 1\n",
    "print(\"Dataset3 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ds1 f1 macro = 0.99\n",
      "Ds1 f1 micro = 0.99\n",
      "Ds1 f1 weighted = 0.99\n",
      "Ds2 f1 macro = 0.92\n",
      "Ds2 f1 micro = 0.96\n",
      "Ds2 f1 weighted = 0.96\n",
      "Ds3 f1 macro = 0.99\n",
      "Ds3 f1 micro = 1.00\n",
      "Ds3 f1 weighted = 1.00\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_predict, average='macro')  \n",
    "print('Ds1 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test, y_predict, average='micro')  \n",
    "print('Ds1 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test, y_predict, average='weighted')  \n",
    "print('Ds1 f1 weighted = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='macro')  \n",
    "print('Ds2 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='micro')  \n",
    "print('Ds2 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test1, y_predict1, average='weighted')  \n",
    "print('Ds2 f1 weighted = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='macro')  \n",
    "print('Ds3 f1 macro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='micro')  \n",
    "print('Ds3 f1 micro = %.2f' %(f1))\n",
    "f1 = f1_score(y_test2, y_predict2, average='weighted')  \n",
    "print('Ds3 f1 weighted = %.2f' %(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ds1 False positive:  30 ; 0.4448398576512456 %\n",
      "Ds2 False positive:  28 ; 2.5112107623318383 %\n",
      "Ds3 False positive:  5 ; 0.4830917874396135 %\n"
     ]
    }
   ],
   "source": [
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test.iteritems():\n",
    "    if y_predict[idx]!=item and y_predict[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds1 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test1.iteritems():\n",
    "    if y_predict1[idx]!=item and y_predict1[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds2 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "fp = 0\n",
    "idx = 0\n",
    "score = 0\n",
    "for _,item in y_test2.iteritems():\n",
    "    if y_predict2[idx]!=item and y_predict2[idx]==\"spam\":\n",
    "        fp+=1\n",
    "    idx += 1\n",
    "print(\"Ds3 False positive: \",fp,\";\",(fp/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on tests, Bayes classifier has good performance when trained on general data, but has worse performance on different class data. The Support Vector Machine has better performance on all data, but it takes a lot longer to train. However Bayesian model can be tested with different marigin to improve it's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2 has highest probability of false positive, so for this dataset it's best to test which marigin has lowest false positive chance and how it'll impact overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 :Ds2 False positive:  114 ; 10.22421524663677 %\n",
      "5 :Dataset2 correct guesses: 975 / 1115 : 87.4439461883408 %\n",
      "6 :Ds2 False positive:  56 ; 5.0224215246636765 %\n",
      "6 :Dataset2 correct guesses: 1023 / 1115 : 91.74887892376682 %\n",
      "7 :Ds2 False positive:  24 ; 2.1524663677130045 %\n",
      "7 :Dataset2 correct guesses: 1041 / 1115 : 93.36322869955157 %\n",
      "8 :Ds2 False positive:  9 ; 0.8071748878923767 %\n",
      "8 :Dataset2 correct guesses: 1030 / 1115 : 92.37668161434978 %\n",
      "9 :Ds2 False positive:  2 ; 0.17937219730941703 %\n",
      "9 :Dataset2 correct guesses: 996 / 1115 : 89.32735426008969 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,10):\n",
    "    y_pred_diff = (model.predict_proba(X_test1)[:,1] >= i/10).astype(bool)\n",
    "    fp = 0\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test1.iteritems():\n",
    "        if item == \"ham\" and y_pred_diff[idx]==True:\n",
    "            fp+=1\n",
    "        idx += 1\n",
    "    print(i,\":Ds2 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test1.iteritems():\n",
    "        if(y_pred_diff[idx]==True and item==\"spam\") or (y_pred_diff[idx]==False and item==\"ham\"):\n",
    "            score+=1\n",
    "        idx += 1\n",
    "    print(i,\":Dataset2 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with marigin 0.8 showed false positive probability less than 1% and overall accuracy of 92,3% which is better than prediction with default marigin. But it's better to test marigins on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 :Ds1 False positive:  12 ; 0.1779359430604982 %\n",
      "5 :Dataset1 correct guesses: 6630 / 6744 : 98.30960854092527 %\n",
      "6 :Ds1 False positive:  8 ; 0.11862396204033215 %\n",
      "6 :Dataset1 correct guesses: 6549 / 6744 : 97.10854092526691 %\n",
      "7 :Ds1 False positive:  6 ; 0.0889679715302491 %\n",
      "7 :Dataset1 correct guesses: 6445 / 6744 : 95.56642941874259 %\n",
      "8 :Ds1 False positive:  4 ; 0.05931198102016608 %\n",
      "8 :Dataset1 correct guesses: 6255 / 6744 : 92.7491103202847 %\n",
      "9 :Ds1 False positive:  2 ; 0.02965599051008304 %\n",
      "9 :Dataset1 correct guesses: 5873 / 6744 : 87.08481613285883 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,10):\n",
    "    y_pred_diff = (model.predict_proba(X_test)[:,1] >= i/10).astype(bool)\n",
    "    fp = 0\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test.iteritems():\n",
    "        if item == \"ham\" and y_pred_diff[idx]==True:\n",
    "            fp+=1\n",
    "        idx += 1\n",
    "    print(i,\":Ds1 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test.iteritems():\n",
    "        if(y_pred_diff[idx]==True and item==\"spam\") or (y_pred_diff[idx]==False and item==\"ham\"):\n",
    "            score+=1\n",
    "        idx += 1\n",
    "    print(i,\":Dataset1 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 :Ds3 False positive:  8 ; 0.7729468599033816 %\n",
      "5 :Dataset3 correct guesses: 1014 / 1035 : 97.97101449275362 %\n",
      "6 :Ds3 False positive:  6 ; 0.5797101449275363 %\n",
      "6 :Dataset3 correct guesses: 1007 / 1035 : 97.29468599033815 %\n",
      "7 :Ds3 False positive:  4 ; 0.3864734299516908 %\n",
      "7 :Dataset3 correct guesses: 1000 / 1035 : 96.61835748792271 %\n",
      "8 :Ds3 False positive:  4 ; 0.3864734299516908 %\n",
      "8 :Dataset3 correct guesses: 983 / 1035 : 94.97584541062803 %\n",
      "9 :Ds3 False positive:  0 ; 0.0 %\n",
      "9 :Dataset3 correct guesses: 938 / 1035 : 90.6280193236715 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(5,10):\n",
    "    y_pred_diff = (model.predict_proba(X_test2)[:,1] >= i/10).astype(bool)\n",
    "    fp = 0\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test2.iteritems():\n",
    "        if item == \"ham\" and y_pred_diff[idx]==True:\n",
    "            fp+=1\n",
    "        idx += 1\n",
    "    print(i,\":Ds3 False positive: \",fp,\";\",(fp/idx)*100,\"%\")\n",
    "    idx = 0\n",
    "    score = 0\n",
    "    for _,item in y_test2.iteritems():\n",
    "        if(y_pred_diff[idx]==True and item==\"spam\") or (y_pred_diff[idx]==False and item==\"ham\"):\n",
    "            score+=1\n",
    "        idx += 1\n",
    "    print(i,\":Dataset3 correct guesses:\",score,\"/\",idx,\":\",(score/idx)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other datasets increasing marigin to 0.8 showed less effect on false positive probability, while decreasing overall accuracy by 3-4%. But it's more important for this classifier to avoid false positives, rather than have high accuracy, so using Bayes classifier with marigin 0.8 will offer best performance. Also using Bayes classifier will allow to dynamically train the model with new data to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of this project was implemented a classifer capable of identifying both email and SMS spam with 90% probability and less than 0.5% of false positive. Such parameters are optimal as it'll be able to filter most of spam while minimizing important message loss. Furthermore, this classifier can be further trained on more data to maintain relevance on newer spam and mails. And due to fast training speed it can be trained on larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference to video: [here](https://drive.google.com/file/d/1NXqC4pt1FMOze3zgpDKphU4vVAd5Be58/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f809b89e4d25a612390917a6d6b87999689c260ba983e2d29d43f0395a837da"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
